{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_NMT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ldselvera/Transformer-NMT/blob/main/Transformer_NMT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfWkC6PmnIrj"
      },
      "source": [
        "##Libraries installations in case they are missing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwSyNcWcr2W4",
        "outputId": "087775a6-cd9e-4f99-fdb4-be9b07ae919c"
      },
      "source": [
        "#Install torchtext version 0.6.0 for Bleu metrics\n",
        "#Otherwise, from torchtext.data.metrics import bleu_score will create an error\n",
        "\n",
        "!pip install torchtext==0.6.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.18.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.94 torchtext-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph5IcMNrQGCo",
        "outputId": "e0ff60c8-8241-43c7-ada4-0b6499b4ba59"
      },
      "source": [
        "# To install spacy languages:\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 661kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.11.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907056 sha256=9963e8a85ff7948541fb4a5ae68d7fcc9b4a06755b68144ad739b5b9c65d5564\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d9jk2_e_/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyUXPHPxUbv4"
      },
      "source": [
        "import torch\n",
        "import spacy\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext import datasets\n",
        "from torchtext.data import Field, BucketIterator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z64auBhfDSCQ"
      },
      "source": [
        "##Detect device, for faster training use GPU, for instance with Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlVZ77MIDNio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64ecd192-1bd7-4aae-89da-452aac24ca7e"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwC2Q032-DtK"
      },
      "source": [
        "##BLEU (Bilingual Evaluation Understudy) compares the machine-written translation to one or several human-written translations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLN9pYzAU5b3"
      },
      "source": [
        "def bleu(data, model, german, english, device, max_length=50, trans=trans):\n",
        "    targets = []\n",
        "    preds = []\n",
        "\n",
        "    for instance in data:\n",
        "        #get source and target sentences\n",
        "        src = vars(instance)[\"src\"]\n",
        "        trg = vars(instance)[\"trg\"]\n",
        "\n",
        "        #model translation\n",
        "        prediction = translate_sentence(model, src, german, english, device,max_length=50, trans=trans)\n",
        "        \n",
        "        #remove <eos> token\n",
        "        prediction = prediction[:-1]  \n",
        "\n",
        "        #store prediction and actual translation for scoring purposes\n",
        "        targets.append([trg])\n",
        "        preds.append(prediction)\n",
        "\n",
        "    return bleu_score(preds, targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Afdm2HAm0r"
      },
      "source": [
        "##Save or load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvavxfqrU6R2"
      },
      "source": [
        "#Save model to pth.tar file\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"Saving checkpoint\")\n",
        "    torch.save(state, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m4ad1nXU9c-"
      },
      "source": [
        "#Load model to pth.tar file\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKhT4vPIBMOZ"
      },
      "source": [
        "##Tokenize english and german"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPL3dSTVVhzf"
      },
      "source": [
        "def tokenize_eng(text):\n",
        "    return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
        "def tokenize_ger(text):\n",
        "    return [tok.text for tok in spacy_ger.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww27RhMrBVy5"
      },
      "source": [
        "##Model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxqySz7sWAsW"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embedding_size, src_vocab_size, trg_vocab_size, src_pad_idx, num_heads, num_encoder, num_decoder, feedforward, dropout, max_len, device,):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        #embded input sentence and positional encoding\n",
        "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
        "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
        "        \n",
        "        #embded output sentence and positional encoding\n",
        "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
        "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        #initialize torch's Transformer\n",
        "        self.transformer = nn.Transformer(embedding_size, num_heads, num_encoder, num_decoder, feedforward, dropout,)\n",
        "        #final linear transformation for outputs\n",
        "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
        "        #dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        #padding for input index\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        #masking to avoid model looking ahead\n",
        "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_length, N = src.shape\n",
        "        trg_length, N = trg.shape\n",
        "\n",
        "        #get positional encoding\n",
        "        src_positions = (torch.arange(0, src_length).unsqueeze(1).expand(src_length, N).to(self.device))\n",
        "        trg_positions = (torch.arange(0, trg_length).unsqueeze(1).expand(trg_length, N).to(self.device))\n",
        "\n",
        "        #get embeddings for input  and output with drop\n",
        "        embed_src = self.dropout((self.src_word_embedding(src) + self.src_position_embedding(src_positions)))\n",
        "        embed_trg = self.dropout((self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions)))\n",
        "\n",
        "        #apply mask\n",
        "        src_padding_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_length).to(self.device)\n",
        "\n",
        "        #input data to model\n",
        "        out = self.transformer(embed_src, embed_trg, src_key_padding_mask=src_padding_mask, tgt_mask=trg_mask,)\n",
        "        \n",
        "        #obtain final predictions from last layer previously defined\n",
        "        out = self.fc_out(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8AIkK-R-JOA"
      },
      "source": [
        "##To translate from English to German, run \"en_de\"\n",
        "##To translate from  German to English, run \"de_de\", by uncommenting \"trasn = \"de_en\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akHp_o89-AEW"
      },
      "source": [
        "#Translate from english to german\n",
        "trans = \"en_de\"\n",
        "\n",
        "#Translate from german to english\n",
        "# trans = \"de_en\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmD2eWtYB5aR"
      },
      "source": [
        "##Data preprocessing:\n",
        "###Tokenize each of the sentences in the Translation Dataset based on the tokenizer defined in the Field"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19lXV3ISWtbH"
      },
      "source": [
        "#Load english and german language from Spacy\n",
        "spacy_eng = spacy.load(\"en\")\n",
        "spacy_ger = spacy.load(\"de\")\n",
        "\n",
        "#Define datatype with instruction to covert to tensor\n",
        "#Set tokenization function, token that will be preprended,\n",
        "#token that will be appended, and set sentence to all lowercase\n",
        "english = Field(tokenize=tokenize_eng, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, )\n",
        "german = Field(tokenize=tokenize_ger, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True)\n",
        "\n",
        "#Create dataset objects for splits of the Multi30k dataset\n",
        "#exts sets extension path of language\n",
        "#fields contains the fields that wil be used for data in each language (from previous lines)\n",
        "if trans == \"en_de\":\n",
        "  train_data, valid_data, test_data = datasets.Multi30k.splits(exts = (\".en\", \".de\"), fields=(english, german))\n",
        "elif trans == \"de_en\":\n",
        "  train_data, valid_data, test_data = datasets.Multi30k.splits(exts = (\".de\", \".en\"), fields=(german, english))\n",
        "else:\n",
        "  print(\"Please go to previous cell and choose between en_de or de_en\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp-nTnP0_eDZ"
      },
      "source": [
        "###The build_vocab method now allows us to create the vocabulary associated with each language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xmeoAks_POx"
      },
      "source": [
        "#Build the vocabulary so we can convert tokens/words into integer\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "\n",
        "#Get size of vocabulary\n",
        "if trans == \"en_de\":\n",
        "  src_vocab_size = len(english.vocab)\n",
        "  trg_vocab_size= len(german.vocab)\n",
        "elif trans == \"de_en\":\n",
        "  src_vocab_size = len(german.vocab)\n",
        "  trg_vocab_size = len(english.vocab)\n",
        "else:\n",
        "  print(\"Please go to previous cell and choose between en_de or de_en\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CjeZ0k6Bm-6"
      },
      "source": [
        "###SRC.vocab.stoi is now a dictionary with the tokens in the vocabulary as keys and their corresponding indices as values\n",
        "###SRC.vocab.itos is the same dictionary with the keys and values swapped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukMEksc_DgFo"
      },
      "source": [
        "##Split data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIHFjMs9B-Wb"
      },
      "source": [
        "###Torchtext feature BucketIterator takes a TranslationDataset as its first argument. It defines an iterator that batches examples of similar lengths together. Minimizes amount of padding needed while producing freshly shuffled batches for each new epoch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY9Hh_idXuA_"
      },
      "source": [
        "batch_size = 128\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), batch_size=batch_size, sort_within_batch=True, sort_key=lambda x: len(x.src), device=device,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1DvPUVvC_pR"
      },
      "source": [
        "##Model hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ31O3XPXLXP"
      },
      "source": [
        "#Input size into the model\n",
        "embedding_size = 512\n",
        "\n",
        "#Number of multihead attentions\n",
        "num_heads = 8\n",
        "\n",
        "#Number of encoders and decoders\n",
        "num_encoder = 3\n",
        "num_decoder = 3\n",
        "dropout = 0.10\n",
        "\n",
        "#Max lenght of each sentence\n",
        "max_len = 100\n",
        "feedforward = 4\n",
        "learning_rate = 3e-4\n",
        "\n",
        "if trans == \"en_de\":\n",
        "  src_pad_idx = german.vocab.stoi[\"<pad>\"]\n",
        "elif trans == \"de_en\":\n",
        "  src_pad_idx = english.vocab.stoi[\"<pad>\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHAI23o0DjVv"
      },
      "source": [
        "#Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fcZIJZgXvUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aa1bd4c-e8b4-42f6-ebb0-e072ea3cc4c7"
      },
      "source": [
        "#Initialize model with model hyperparameters\n",
        "model = Transformer(embedding_size, src_vocab_size, trg_vocab_size, src_pad_idx, num_heads, num_encoder, num_decoder, feedforward, dropout, max_len, device,).to(device)\n",
        "#Set Adam optimizer with learning \n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "if trans == \"en_de\":\n",
        "  pad_idx = german.vocab.stoi[\"<pad>\"]\n",
        "elif trans == \"de_en\":\n",
        "  pad_idx = english.vocab.stoi[\"<pad>\"]\n",
        "\n",
        "#tell the nn.CrossEntropyLoss function to ignore the indices where the target is simply padding\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "def count_parameters(model: nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 20,671,687 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUmrVi6uDsuA"
      },
      "source": [
        "##Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dx7spKlPdX4"
      },
      "source": [
        "def translate_sentence(model, sentence, german, english, device, max_length, trans):\n",
        "    # Load tokenizer\n",
        "    if trans == \"en_de\":\n",
        "      spacy_eng = spacy.load(\"en\")\n",
        "    elif trans == \"de_en\":\n",
        "      spacy_ger = spacy.load(\"de\")\n",
        "\n",
        "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
        "    if type(sentence) == str:\n",
        "        if trans == \"en_de\":\n",
        "          tokens = [token.text.lower() for token in spacy_eng(sentence)]\n",
        "        elif trans == \"de_en\":\n",
        "          tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
        "        # tokens = [token.text.lower() for token in spacy_fr(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    # Add <SOS> and <EOS> in beginning and end respectively\n",
        "    if trans == \"en_de\":\n",
        "      tokens.insert(0, english.init_token)\n",
        "      tokens.append(english.eos_token)\n",
        "      # Go through each german token and convert to an index\n",
        "      text_to_indices = [english.vocab.stoi[token] for token in tokens]\n",
        "    elif trans == \"de_en\":\n",
        "      tokens.insert(0, german.init_token)\n",
        "      tokens.append(german.eos_token)\n",
        "      # Go through each german token and convert to an index\n",
        "      text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    # Convert to Tensor\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    if trans == \"en_de\":\n",
        "      outputs = [german.vocab.stoi[\"<sos>\"]]\n",
        "    elif trans == \"de_en\":\n",
        "      outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "\n",
        "    for i in range(max_length):\n",
        "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(sentence_tensor, trg_tensor)\n",
        "\n",
        "        best_guess = output.argmax(2)[-1, :].item()\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        if trans == \"en_de\":\n",
        "          if best_guess == german.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "        elif trans == \"de_en\":\n",
        "          if best_guess == english.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    if trans == \"en_de\":\n",
        "      translated_sentence = [german.vocab.itos[idx] for idx in outputs]\n",
        "    elif trans == \"de_en\":\n",
        "      translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "\n",
        "    # remove start token        \n",
        "    return translated_sentence[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m-Ks-TPD4XQ"
      },
      "source": [
        "##Load model or save model \n",
        "##IMPORTANT: Set the path to the model file (which was provided to you)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGB3jfu_Xm1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edb46df4-12bb-45d7-c232-420f128c9205"
      },
      "source": [
        "load_model = True\n",
        "save_model = False\n",
        "\n",
        "#Modify this paths according\n",
        "if load_model:\n",
        "    if trans == \"en_de\":\n",
        "      load_checkpoint(torch.load(\"en_de.pth.tar\"), model, optimizer)\n",
        "    elif trans == \"de_en\":\n",
        "      load_checkpoint(torch.load(\"de_en.pth.tar\"), model, optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4yHTy0rUOXi"
      },
      "source": [
        "##Record time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8QmXu-oULvi"
      },
      "source": [
        "def epoch_time(start_time: int, end_time: int):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cH7c01n962k"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE2dAm0yXgqP"
      },
      "source": [
        "def train(mode: nn.Module, iterator: BucketIterator, optimizer: optim.Optimizer, criterio: nn.Module, clip:float):\n",
        "      model.train()\n",
        "      epoch_loss = 0\n",
        "      losses = []\n",
        "\n",
        "      #ButcketIterators can be called just like DataLoader\n",
        "      for batch_idx, batch in enumerate(iterator):\n",
        "          #Get input and targets and get to cuda\n",
        "          #Each batch then has src and trg attributes\n",
        "          inp_data = batch.src.to(device)\n",
        "          target = batch.trg.to(device)\n",
        "\n",
        "          # Forward propagation\n",
        "          output = model(inp_data, target[:-1])\n",
        "\n",
        "          output = output.reshape(-1, output.shape[2])\n",
        "          target = target[1:].reshape(-1)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          loss = criterion(output, target)\n",
        "          losses.append(loss.item())\n",
        "\n",
        "          # Back propagation\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip to avoid exploding gradient issues\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "          # Gradient descent step\n",
        "          optimizer.step()\n",
        "\n",
        "          epoch_loss += loss.item()\n",
        "\n",
        "      mean_loss = sum(losses) / len(losses)\n",
        "\n",
        "      return epoch_loss / len(iterator)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq7rlx1svsra"
      },
      "source": [
        "##Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs65ZenDWJdT"
      },
      "source": [
        "def evaluate(model: nn.Module, iterator: BucketIterator, criterion: nn.Module):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(iterator):\n",
        "\n",
        "            inp_data = batch.src.to(device)\n",
        "            target = batch.trg.to(device)\n",
        "\n",
        "            output = model(inp_data, target[:-1])\n",
        "            # output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            output = output.reshape(-1, output.shape[2])\n",
        "            target = target[1:].reshape(-1)            \n",
        "\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq1qhy9p-YrM"
      },
      "source": [
        "##Choose number of epochs:\n",
        "###For 5 epochs it takes ~2-minutes\n",
        "###For 10 epochs it takes ~4-minutes\n",
        "###For 50 epochs it takes ~20-minutes with GPU\n",
        "###For 100 epochs it takes ~1-hour with GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD-J9haT-YIt"
      },
      "source": [
        "num_epochs = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYY1O_kaO9Sy",
        "outputId": "95b90d7b-c52b-4c44-ad51-0a0735358223"
      },
      "source": [
        "CLIP = 1\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    if save_model:\n",
        "      checkpoint = { \"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict(),}\n",
        "      save_checkpoint(checkpoint)\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    if num_epochs < 11:\n",
        "      print(f'Epoch: {epoch+1:02}')\n",
        "      print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "      print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
        "    else:\n",
        "      if(epoch % 5 == 0):\n",
        "        print(f'Epoch: {epoch+1:02}')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
        "\n",
        "end_time = time.time()\n",
        "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "print(f'Training Time: {epoch_mins}m {epoch_secs}s')\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "print(f'| Test Loss: {test_loss:.3f}')\n",
        "\n",
        "# Specify a path\n",
        "PATH = trans + \"_model.pt\"\n",
        "\n",
        "# Save\n",
        "torch.save(model, PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 0.073\n",
            "\t Val. Loss: 2.404\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.073\n",
            "\t Val. Loss: 2.422\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.071\n",
            "\t Val. Loss: 2.444\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.071\n",
            "\t Val. Loss: 2.433\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.071\n",
            "\t Val. Loss: 2.430\n",
            "Training Time: 1m 31s\n",
            "| Test Loss: 2.599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3ayqPbMWHKl"
      },
      "source": [
        "num_epochs = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KVgIT1mUG3a",
        "outputId": "1dc2426f-23cf-4462-dcdc-93a1a0838c1f"
      },
      "source": [
        "CLIP = 1\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    if save_model:\n",
        "      checkpoint = { \"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict(),}\n",
        "      save_checkpoint(checkpoint)\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    if(epoch % 5 == 0):\n",
        "      print(f'Epoch: {epoch+1:02}')\n",
        "      print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "      print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
        "\n",
        "end_time = time.time()\n",
        "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "print(f'Training Time: {epoch_mins}m {epoch_secs}s')\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "print(f'| Test Loss: {test_loss:.3f}')\n",
        "\n",
        "# Specify a path\n",
        "PATH = trans + \"_model.pt\"\n",
        "\n",
        "# Save\n",
        "torch.save(model, PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 0.068\n",
            "\t Val. Loss: 2.465\n",
            "Epoch: 11\n",
            "\tTrain Loss: 0.063\n",
            "\t Val. Loss: 2.536\n",
            "Epoch: 21\n",
            "\tTrain Loss: 0.058\n",
            "\t Val. Loss: 2.600\n",
            "Epoch: 31\n",
            "\tTrain Loss: 0.054\n",
            "\t Val. Loss: 2.654\n",
            "Epoch: 41\n",
            "\tTrain Loss: 0.051\n",
            "\t Val. Loss: 2.704\n",
            "Training Time: 23m 43s\n",
            "| Test Loss: 2.933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub9y9Q24gBmy"
      },
      "source": [
        "##Translation performance metric BLEU\n",
        "## WARNING: Execute only if you have installed torchtext 0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxNwhP9XXa3f"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "#set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# running on entire test data takes a while\n",
        "score = bleu(test_data[1:100], model, german, english, device, max_length=50, trans=trans)\n",
        "print(f\"Bleu score %.2f\" % (score * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DAAq5Bjozjo"
      },
      "source": [
        "##Sample runs: English to German"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_qxy2cuvy1r"
      },
      "source": [
        "###Only execute if current translation is set to \"en_de\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dn-eH2bRv4y6",
        "outputId": "1c080918-88dc-4b49-be14-4dec0b470252"
      },
      "source": [
        "#check what current translation is set to\n",
        "trans"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'en_de'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VLGHjZHI3MH",
        "outputId": "55b5a0e0-5b41-45b0-d581-bc4f7c8a11c0"
      },
      "source": [
        "sentence = vars(test_data[0])[\"src\"]\n",
        "translated_sentence = translate_sentence( model, sentence, german, english, device, max_length=50, trans=trans)\n",
        "print(\"English: \", sentence)\n",
        "print(\"Model translation: \", translated_sentence)\n",
        "print(\"Actual translation: \", vars(test_data[0])[\"trg\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English:  ['a', 'man', 'in', 'an', 'orange', 'hat', 'starring', 'at', 'something', '.']\n",
            "Model translation:  ['ein', 'mann', 'mit', 'orangefarbener', 'mütze', 'schaut', 'irgendetwas', 'an', '.', '<eos>']\n",
            "Actual translation:  ['ein', 'mann', 'mit', 'einem', 'orangefarbenen', 'hut', ',', 'der', 'etwas', 'anstarrt', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ3nhToSpCxQ",
        "outputId": "b5ab0121-bccc-4657-97b2-3e8e00b99e8f"
      },
      "source": [
        "sentence = vars(test_data[1])[\"src\"]\n",
        "translated_sentence = translate_sentence( model, sentence, german, english, device, max_length=50, trans=trans)\n",
        "print(\"English: \", sentence)\n",
        "print(\"Model translation: \", translated_sentence)\n",
        "print(\"Actual translation: \", vars(test_data[1])[\"trg\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English:  ['a', 'boston', 'terrier', 'is', 'running', 'on', 'lush', 'green', 'grass', 'in', 'front', 'of', 'a', 'white', 'fence', '.']\n",
            "Model translation:  ['ein', 'fan', 'rennt', 'auf', 'einer', '<unk>', 'grünen', 'wiese', 'vor', 'einem', 'weißen', 'zaun', '.', '<eos>']\n",
            "Actual translation:  ['ein', 'boston', 'terrier', 'läuft', 'über', 'saftig-grünes', 'gras', 'vor', 'einem', 'weißen', 'zaun', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNniL-BAEMBQ"
      },
      "source": [
        "##Sample runs: German to English"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_I66I-iv7qP"
      },
      "source": [
        "###Only execute if current translation is set to \"de_en\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TP1WTLa8wBJd",
        "outputId": "bac93ac4-263b-4699-f70f-34fd9b2a9a57"
      },
      "source": [
        "#check what current translation is set to\n",
        "trans"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'en_de'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOIe0Yrr6lNF",
        "outputId": "a8fa398c-418c-4c15-ca44-6ddadad66c81"
      },
      "source": [
        "sentence = vars(test_data[0])[\"src\"]\n",
        "translated_sentence = translate_sentence( model, sentence, german, english, device, max_length=50, trans=trans)\n",
        "print(\"German: \", sentence)\n",
        "print(\"Model translation: \", translated_sentence)\n",
        "print(\"Actual translation: \", vars(test_data[0])[\"trg\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "German:  ['ein', 'mann', 'mit', 'einem', 'orangefarbenen', 'hut', ',', 'der', 'etwas', 'anstarrt', '.']\n",
            "Model translation:  ['a', 'man', 'in', 'an', 'orange', 'hat', 'welding', 'something', '.', '<eos>']\n",
            "Actual translation:  ['a', 'man', 'in', 'an', 'orange', 'hat', 'starring', 'at', 'something', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAJhdaqV4OZq",
        "outputId": "5d1fed54-d4b9-46f7-be4b-9ad053f5d5de"
      },
      "source": [
        "sentence = vars(test_data[1])[\"src\"]\n",
        "translated_sentence = translate_sentence( model, sentence, german, english, device, max_length=50, trans=trans)\n",
        "print(\"German: \", sentence)\n",
        "print(\"Model translation: \", translated_sentence)\n",
        "print(\"Actual translation: \", vars(test_data[1])[\"trg\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "German:  ['ein', 'boston', 'terrier', 'läuft', 'über', 'saftig-grünes', 'gras', 'vor', 'einem', 'weißen', 'zaun', '.']\n",
            "Model translation:  ['a', 'boston', '<unk>', 'terrier', 'runs', 'over', 'grass', 'in', 'front', 'of', 'a', 'white', 'fence', '.', '<eos>']\n",
            "Actual translation:  ['a', 'boston', 'terrier', 'is', 'running', 'on', 'lush', 'green', 'grass', 'in', 'front', 'of', 'a', 'white', 'fence', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAZWGx2jFiHX",
        "outputId": "c880baad-f192-4c51-aa00-e06ed0cec08d"
      },
      "source": [
        "sentence = \"ein pferd geht unter einer brücke neben einem boot.\"\n",
        "translated_sentence = translate_sentence( model, sentence, german, english, device, max_length=50, trans=trans)\n",
        "print(\"German: \", sentence)\n",
        "print(\"Model translation: \", translated_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "German:  ein pferd geht unter einer brücke neben einem boot.\n",
            "Model translation:  ['a', 'horse', 'is', 'walking', 'beside', 'a', 'boat', 'under', 'a', 'bridge', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}