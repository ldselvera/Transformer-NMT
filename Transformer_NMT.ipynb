{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_NMT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ldselvera/Transformer-NMT/blob/main/Transformer_NMT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfWkC6PmnIrj"
      },
      "source": [
        "##Libraries installations in case they are missing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwSyNcWcr2W4",
        "outputId": "5015ac85-834c-46ba-c540-94684b750d09"
      },
      "source": [
        "#Install torchtext version 0.6.0 for Bleu metrics\n",
        "#Otherwise, from torchtext.data.metrics import bleu_score will create an error\n",
        "\n",
        "!pip install torchtext==0.6.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 23.1MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 30.3MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40kB 18.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 51kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 61kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.8.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 14.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.9.1\n",
            "    Uninstalling torchtext-0.9.1:\n",
            "      Successfully uninstalled torchtext-0.9.1\n",
            "Successfully installed sentencepiece-0.1.95 torchtext-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph5IcMNrQGCo",
        "outputId": "05fd1d6d-cce0-498e-8fb9-a5dd117aec92"
      },
      "source": [
        "# To install spacy languages:\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (56.1.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (56.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.1)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp37-none-any.whl size=14907057 sha256=76f55a7094aa50f188b2e69fc4e6c01b9579adea24f2ac04e9295f0a94758232\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_cb_pnfl/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyUXPHPxUbv4"
      },
      "source": [
        "import torch\n",
        "import spacy\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchtext import datasets\n",
        "from torchtext.data import Field, BucketIterator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z64auBhfDSCQ"
      },
      "source": [
        "##Detect device, for faster training use GPU, for instance with Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlVZ77MIDNio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49a02d37-734a-4ddd-b4d7-9d1b291bbd01"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwC2Q032-DtK"
      },
      "source": [
        "##BLEU (Bilingual Evaluation Understudy) compares the machine-written translation to one or several human-written translations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLN9pYzAU5b3"
      },
      "source": [
        "def bleu(data, model, german, english, device):\n",
        "    targets = []\n",
        "    preds = []\n",
        "\n",
        "    for instance in data:\n",
        "        #get source and target sentences\n",
        "        src = vars(instance)[\"src\"]\n",
        "        trg = vars(instance)[\"trg\"]\n",
        "\n",
        "        #model translation\n",
        "        prediction = translate_sentence(model, src, german, english, device)\n",
        "        \n",
        "        #remove <eos> token\n",
        "        prediction = prediction[:-1]  \n",
        "\n",
        "        #store prediction and actual translation for scoring purposes\n",
        "        targets.append([trg])\n",
        "        preds.append(prediction)\n",
        "\n",
        "    return bleu_score(preds, targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Afdm2HAm0r"
      },
      "source": [
        "##Save or load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvavxfqrU6R2"
      },
      "source": [
        "#Save model to pth.tar file\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"Saving checkpoint\")\n",
        "    torch.save(state, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m4ad1nXU9c-"
      },
      "source": [
        "#Load model to pth.tar file\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKhT4vPIBMOZ"
      },
      "source": [
        "##Tokenize english and german"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPL3dSTVVhzf"
      },
      "source": [
        "def tokenize_eng(text):\n",
        "    return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
        "def tokenize_ger(text):\n",
        "    return [tok.text for tok in spacy_ger.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww27RhMrBVy5"
      },
      "source": [
        "##Model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxqySz7sWAsW"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embedding_size, src_vocab_size, trg_vocab_size, src_pad_idx, num_heads, num_encoder, num_decoder, feedforward, dropout, max_len, device,):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        #embded input sentence and positional encoding\n",
        "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
        "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
        "        \n",
        "        #embded output sentence and positional encoding\n",
        "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
        "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        #initialize torch's Transformer\n",
        "        self.transformer = nn.Transformer(embedding_size, num_heads, num_encoder, num_decoder, feedforward, dropout,)\n",
        "        #final linear transformation for outputs\n",
        "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
        "        #dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        #padding for input index\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        #masking to avoid model looking ahead\n",
        "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_length, N = src.shape\n",
        "        trg_length, N = trg.shape\n",
        "\n",
        "        #get positional encoding\n",
        "        src_positions = (torch.arange(0, src_length).unsqueeze(1).expand(src_length, N).to(self.device))\n",
        "        trg_positions = (torch.arange(0, trg_length).unsqueeze(1).expand(trg_length, N).to(self.device))\n",
        "\n",
        "        #get embeddings for input  and output with drop\n",
        "        embed_src = self.dropout((self.src_word_embedding(src) + self.src_position_embedding(src_positions)))\n",
        "        embed_trg = self.dropout((self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions)))\n",
        "\n",
        "        #apply mask\n",
        "        src_padding_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_length).to(self.device)\n",
        "\n",
        "        #input data to model\n",
        "        out = self.transformer(embed_src, embed_trg, src_key_padding_mask=src_padding_mask, tgt_mask=trg_mask,)\n",
        "        \n",
        "        #obtain final predictions from last layer previously defined\n",
        "        out = self.fc_out(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG1Zeh0WlVBe"
      },
      "source": [
        "#Input size into the model\n",
        "embedding_size = 512\n",
        "\n",
        "#Number of multihead attentions\n",
        "num_heads = 8\n",
        "\n",
        "#Number of encoders and decoders\n",
        "num_encoder = 3\n",
        "num_decoder = 3\n",
        "dropout = 0.10\n",
        "\n",
        "#Max lenght of each sentence\n",
        "max_len = 100\n",
        "feedforward = 4\n",
        "learning_rate = 3e-4\n",
        "\n",
        "src_vocab_size = 29\n",
        "trg_vocab_size = 29\n",
        "src_pad_idx = 0\n",
        "pad_idx = 0\n",
        "\n",
        "#Initialize model with model hyperparameters\n",
        "model = Transformer(embedding_size, src_vocab_size, trg_vocab_size, src_pad_idx, num_heads, num_encoder, num_decoder, feedforward, dropout, max_len, device,).to(device)\n",
        "#Set Adam optimizer with learning \n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#tell the nn.CrossEntropyLoss function to ignore the indices where the target is simply padding\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "287718Lcm1WR"
      },
      "source": [
        "def train(mode: nn.Module, iterator: BucketIterator, optimizer: optim.Optimizer, criterio: nn.Module, clip:float):\n",
        "      model.train()\n",
        "      epoch_loss = 0\n",
        "      losses = []\n",
        "\n",
        "      #ButcketIterators can be called just like DataLoader\n",
        "      for batch_idx, batch in enumerate(iterator):\n",
        "          #Get input and targets and get to cuda\n",
        "          #Each batch then has src and trg attributes\n",
        "          inp_data = batch.src.to(device)\n",
        "          target = batch.trg.to(device)\n",
        "\n",
        "          # Forward propagation\n",
        "          output = model(inp_data, target[:-1])\n",
        "\n",
        "          output = output.reshape(-1, output.shape[2])\n",
        "          target = target[1:].reshape(-1)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          loss = criterion(output, target)\n",
        "          losses.append(loss.item())\n",
        "\n",
        "          # Back propagation\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip to avoid exploding gradient issues\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "          # Gradient descent step\n",
        "          optimizer.step()\n",
        "\n",
        "          epoch_loss += loss.item()\n",
        "\n",
        "      mean_loss = sum(losses) / len(losses)\n",
        "\n",
        "      return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_vYU4qCoh5i"
      },
      "source": [
        "class Batch:\n",
        "    \"Object for holding a batch of data with mask during training.\"\n",
        "    def __init__(self, src, trg=None, pad=0):\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2)\n",
        "        if trg is not None:\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask =                 self.make_std_mask(self.trg, pad)\n",
        "            self.ntokens = (self.trg_y != pad).data.sum()\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"Create a mask to hide padding and future words.\"\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "        tgt_mask = tgt_mask & Variable(\n",
        "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "        return tgt_mask\n",
        "\n",
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "def train_gen(filename, V, WINDOW_SIZE, batch, nbatches, log_source):\n",
        "\n",
        "    num_sessions = 0\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "    t1 = torch.from_numpy(np.zeros((batch, WINDOW_SIZE),dtype=int))\n",
        "    t2 = torch.from_numpy(np.zeros((batch, WINDOW_SIZE),dtype=int))\n",
        "\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            num_sessions += 1\n",
        "            line = tuple(map(lambda n: n, map(int, line.strip().split())))\n",
        "\n",
        "            # for i in range(len(line) - WINDOW_SIZE):\n",
        "            inputs.append(line[0:WINDOW_SIZE])\n",
        "            outputs.append(line[WINDOW_SIZE:WINDOW_SIZE*2])\n",
        "\n",
        "    for j in range(nbatches):\n",
        "        for i in range(batch):\n",
        "            x = inputs[i]\n",
        "            y = outputs[i] \n",
        "            \n",
        "            t1[i][0:] = torch.tensor(x, dtype=torch.float).to(device)\n",
        "            t2[i][0:] = torch.tensor(y, dtype=torch.float).to(device)\n",
        "\n",
        "#             t1[:,0] = 0\n",
        "#             t2[:,0] = 0          \n",
        "\n",
        "        \n",
        "        src = Variable(t1, requires_grad=False)\n",
        "        tgt = Variable(t2, requires_grad=False)\n",
        "\n",
        "        \n",
        "        yield Batch(src, tgt, 0) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8AIkK-R-JOA"
      },
      "source": [
        "##To translate from English to German, run \"en_de\"\n",
        "##To translate from  German to English, run \"de_de\", by uncommenting \"trasn = \"de_en\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akHp_o89-AEW"
      },
      "source": [
        "#Translate from english to german\n",
        "# trans = \"en_de\"\n",
        "\n",
        "#Translate from german to english\n",
        "trans = \"de_en\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmD2eWtYB5aR"
      },
      "source": [
        "##Data preprocessing:\n",
        "###Tokenize each of the sentences in the Translation Dataset based on the tokenizer defined in the Field"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19lXV3ISWtbH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ba83264-28d9-4a19-af9c-175ba81b1bc1"
      },
      "source": [
        "#Load english and german language from Spacy\n",
        "spacy_eng = spacy.load(\"en\")\n",
        "spacy_ger = spacy.load(\"de\")\n",
        "\n",
        "#Define datatype with instruction to covert to tensor\n",
        "#Set tokenization function, token that will be preprended,\n",
        "#token that will be appended, and set sentence to all lowercase\n",
        "english = Field(tokenize=tokenize_eng, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, )\n",
        "german = Field(tokenize=tokenize_ger, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True)\n",
        "\n",
        "#Create dataset objects for splits of the Multi30k dataset\n",
        "#exts sets extension path of language\n",
        "#fields contains the fields that wil be used for data in each language (from previous lines)\n",
        "if trans == \"en_de\":\n",
        "  train_data, valid_data, test_data = datasets.Multi30k.splits(exts = (\".en\", \".de\"), fields=(english, german))\n",
        "elif trans == \"de_en\":\n",
        "  train_data, valid_data, test_data = datasets.Multi30k.splits(exts = (\".de\", \".en\"), fields=(german, english))\n",
        "else:\n",
        "  print(\"Please go to previous cell and choose between en_de or de_en\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:01<00:00, 911kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 175kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 164kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp-nTnP0_eDZ"
      },
      "source": [
        "###The build_vocab method now allows us to create the vocabulary associated with each language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xmeoAks_POx"
      },
      "source": [
        "#Build the vocabulary so we can convert tokens/words into integer\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "\n",
        "#Get size of vocabulary\n",
        "if trans == \"en_de\":\n",
        "  src_vocab_size = len(english.vocab)\n",
        "  trg_vocab_size= len(german.vocab)\n",
        "elif trans == \"de_en\":\n",
        "  src_vocab_size = len(german.vocab)\n",
        "  trg_vocab_size = len(english.vocab)\n",
        "else:\n",
        "  print(\"Please go to previous cell and choose between en_de or de_en\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CjeZ0k6Bm-6"
      },
      "source": [
        "###SRC.vocab.stoi is now a dictionary with the tokens in the vocabulary as keys and their corresponding indices as values\n",
        "###SRC.vocab.itos is the same dictionary with the keys and values swapped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukMEksc_DgFo"
      },
      "source": [
        "##Split data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIHFjMs9B-Wb"
      },
      "source": [
        "###Torchtext feature BucketIterator takes a TranslationDataset as its first argument. It defines an iterator that batches examples of similar lengths together. Minimizes amount of padding needed while producing freshly shuffled batches for each new epoch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY9Hh_idXuA_"
      },
      "source": [
        "batch_size = 128\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), batch_size=batch_size, sort_within_batch=True, sort_key=lambda x: len(x.src), device=device,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1DvPUVvC_pR"
      },
      "source": [
        "##Model hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ31O3XPXLXP"
      },
      "source": [
        "if trans == \"en_de\":\n",
        "  src_pad_idx = german.vocab.stoi[\"<pad>\"]\n",
        "elif trans == \"de_en\":\n",
        "  src_pad_idx = english.vocab.stoi[\"<pad>\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry5Ml75nl518"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHAI23o0DjVv"
      },
      "source": [
        "#Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fcZIJZgXvUv"
      },
      "source": [
        "#Initialize model with model hyperparameters\n",
        "model = Transformer(embedding_size, src_vocab_size, trg_vocab_size, src_pad_idx, num_heads, num_encoder, num_decoder, feedforward, dropout, max_len, device,).to(device)\n",
        "#Set Adam optimizer with learning \n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "if trans == \"en_de\":\n",
        "  pad_idx = german.vocab.stoi[\"<pad>\"]\n",
        "elif trans == \"de_en\":\n",
        "  pad_idx = english.vocab.stoi[\"<pad>\"]\n",
        "\n",
        "\n",
        "\n",
        "def count_parameters(model: nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUmrVi6uDsuA"
      },
      "source": [
        "##Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dx7spKlPdX4"
      },
      "source": [
        "def translate_sentence(model, sentence, german, english, device, max_length, trans):\n",
        "    # Load tokenizer\n",
        "    if trans == \"en_de\":\n",
        "      spacy_eng = spacy.load(\"en\")\n",
        "    elif trans == \"de_en\":\n",
        "      spacy_ger = spacy.load(\"de\")\n",
        "\n",
        "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
        "    if type(sentence) == str:\n",
        "        if trans == \"en_de\":\n",
        "          tokens = [token.text.lower() for token in spacy_eng(sentence)]\n",
        "        elif trans == \"de_en\":\n",
        "          tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
        "        # tokens = [token.text.lower() for token in spacy_fr(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    # Add <SOS> and <EOS> in beginning and end respectively\n",
        "    if trans == \"en_de\":\n",
        "      tokens.insert(0, english.init_token)\n",
        "      tokens.append(english.eos_token)\n",
        "      # Go through each german token and convert to an index\n",
        "      text_to_indices = [english.vocab.stoi[token] for token in tokens]\n",
        "    elif trans == \"de_en\":\n",
        "      tokens.insert(0, german.init_token)\n",
        "      tokens.append(german.eos_token)\n",
        "      # Go through each german token and convert to an index\n",
        "      text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    # Convert to Tensor\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    if trans == \"en_de\":\n",
        "      outputs = [german.vocab.stoi[\"<sos>\"]]\n",
        "    elif trans == \"de_en\":\n",
        "      outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "\n",
        "    for i in range(max_length):\n",
        "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(sentence_tensor, trg_tensor)\n",
        "\n",
        "        best_guess = output.argmax(2)[-1, :].item()\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        if trans == \"en_de\":\n",
        "          if best_guess == german.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "        elif trans == \"de_en\":\n",
        "          if best_guess == english.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    if trans == \"en_de\":\n",
        "      translated_sentence = [german.vocab.itos[idx] for idx in outputs]\n",
        "    elif trans == \"de_en\":\n",
        "      translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "\n",
        "    # remove start token        \n",
        "    return translated_sentence[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m-Ks-TPD4XQ"
      },
      "source": [
        "##Load model or save model \n",
        "##IMPORTANT: Set the path to the model file (which was provided to you)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGB3jfu_Xm1G"
      },
      "source": [
        "load_model = True\n",
        "save_model = False\n",
        "\n",
        "#Modify this paths according\n",
        "if load_model:\n",
        "    if trans == \"en_de\":\n",
        "      load_checkpoint(torch.load(\"en_de.pth.tar\"), model, optimizer)\n",
        "    elif trans == \"de_en\":\n",
        "      load_checkpoint(torch.load(\"de_en.pth.tar\"), model, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5GNy7ReS82b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYccTgjXS3eG"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4yHTy0rUOXi"
      },
      "source": [
        "##Record time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8QmXu-oULvi"
      },
      "source": [
        "def epoch_time(start_time: int, end_time: int):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cH7c01n962k"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE2dAm0yXgqP"
      },
      "source": [
        "def train(mode: nn.Module, iterator: BucketIterator, optimizer: optim.Optimizer, criterio: nn.Module, clip:float):\n",
        "      model.train()\n",
        "      epoch_loss = 0\n",
        "      losses = []\n",
        "\n",
        "      #ButcketIterators can be called just like DataLoader\n",
        "      for batch_idx, batch in enumerate(iterator):\n",
        "          #Get input and targets and get to cuda\n",
        "          #Each batch then has src and trg attributes\n",
        "          inp_data = batch.src.to(device)\n",
        "          target = batch.trg.to(device)\n",
        "\n",
        "          # Forward propagation\n",
        "          output = model(inp_data, target[:-1])\n",
        "\n",
        "          output = output.reshape(-1, output.shape[2])\n",
        "          target = target[1:].reshape(-1)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          loss = criterion(output, target)\n",
        "          losses.append(loss.item())\n",
        "\n",
        "          # Back propagation\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip to avoid exploding gradient issues\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "          # Gradient descent step\n",
        "          optimizer.step()\n",
        "\n",
        "          epoch_loss += loss.item()\n",
        "\n",
        "      mean_loss = sum(losses) / len(losses)\n",
        "\n",
        "      return epoch_loss / len(iterator)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq7rlx1svsra"
      },
      "source": [
        "##Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs65ZenDWJdT"
      },
      "source": [
        "def evaluate(model: nn.Module, iterator: BucketIterator, criterion: nn.Module):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(iterator):\n",
        "\n",
        "            inp_data = batch.src.to(device)\n",
        "            target = batch.trg.to(device)\n",
        "\n",
        "            output = model(inp_data, target[:-1])\n",
        "            # output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            output = output.reshape(-1, output.shape[2])\n",
        "            target = target[1:].reshape(-1)            \n",
        "\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq1qhy9p-YrM"
      },
      "source": [
        "##Choose number of epochs:\n",
        "###For 5 epochs it takes ~2-minutes\n",
        "###For 10 epochs it takes ~4-minutes\n",
        "###For 50 epochs it takes ~20-minutes with GPU\n",
        "###For 100 epochs it takes ~1-hour with GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD-J9haT-YIt"
      },
      "source": [
        "num_epochs = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYY1O_kaO9Sy"
      },
      "source": [
        "CLIP = 1\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    if save_model:\n",
        "      checkpoint = { \"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict(),}\n",
        "      save_checkpoint(checkpoint)\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    if num_epochs < 11:\n",
        "      print(f'Epoch: {epoch+1:02}')\n",
        "      print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "      print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
        "    else:\n",
        "      if(epoch % 5 == 0):\n",
        "        print(f'Epoch: {epoch+1:02}')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
        "\n",
        "end_time = time.time()\n",
        "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "print(f'Training Time: {epoch_mins}m {epoch_secs}s')\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "print(f'| Test Loss: {test_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3ayqPbMWHKl"
      },
      "source": [
        "num_epochs = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KVgIT1mUG3a"
      },
      "source": [
        "CLIP = 1\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    if save_model:\n",
        "      checkpoint = { \"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict(),}\n",
        "      save_checkpoint(checkpoint)\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    if(epoch % 5 == 0):\n",
        "      print(f'Epoch: {epoch+1:02}')\n",
        "      print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "      print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
        "\n",
        "end_time = time.time()\n",
        "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "print(f'Training Time: {epoch_mins}m {epoch_secs}s')\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "print(f'| Test Loss: {test_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub9y9Q24gBmy"
      },
      "source": [
        "##Translation performance metric BLEU\n",
        "## WARNING: Execute only if you have installed torchtext 0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxNwhP9XXa3f"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "#set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# running on entire test data takes a while\n",
        "score = bleu(test_data[1:100], model, german, english, device)\n",
        "print(f\"Bleu score %.2f\" % (score * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DAAq5Bjozjo"
      },
      "source": [
        "##Sample runs: English to German"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_qxy2cuvy1r"
      },
      "source": [
        "###Only execute if current translation is set to \"en_de\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn-eH2bRv4y6"
      },
      "source": [
        "#check what current translation is set to\n",
        "trans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VLGHjZHI3MH"
      },
      "source": [
        "sentence = vars(test_data[0])[\"src\"]\n",
        "translated_sentence = translate_sentence( model, sentence, german, english, device, max_length=50, trans=trans)\n",
        "print(\"English: \", sentence)\n",
        "print(\"Model translation: \", translated_sentence)\n",
        "print(\"Actual translation: \", vars(test_data[0])[\"trg\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ3nhToSpCxQ"
      },
      "source": [
        "sentence = vars(test_data[1])[\"src\"]\n",
        "translated_sentence = translate_sentence( model, sentence, german, english, device, max_length=50, trans=trans)\n",
        "print(\"English: \", sentence)\n",
        "print(\"Model translation: \", translated_sentence)\n",
        "print(\"Actual translation: \", vars(test_data[1])[\"trg\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNniL-BAEMBQ"
      },
      "source": [
        "##Sample runs: German to English"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_I66I-iv7qP"
      },
      "source": [
        "###Only execute if current translation is set to \"de_en\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP1WTLa8wBJd"
      },
      "source": [
        "#check what current translation is set to\n",
        "trans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOIe0Yrr6lNF"
      },
      "source": [
        "sentence = vars(test_data[0])[\"src\"]\n",
        "translated_sentence = translate_sentence( model, sentence, german, english, device, max_length=50, trans=trans)\n",
        "print(\"German: \", sentence)\n",
        "print(\"Model translation: \", translated_sentence)\n",
        "print(\"Actual translation: \", vars(test_data[0])[\"trg\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAJhdaqV4OZq"
      },
      "source": [
        "sentence = vars(test_data[1])[\"src\"]\n",
        "translated_sentence = translate_sentence( model, sentence, german, english, device, max_length=50, trans=trans)\n",
        "print(\"German: \", sentence)\n",
        "print(\"Model translation: \", translated_sentence)\n",
        "print(\"Actual translation: \", vars(test_data[1])[\"trg\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAZWGx2jFiHX"
      },
      "source": [
        "sentence = \"ein pferd geht unter einer brücke neben einem boot.\"\n",
        "translated_sentence = translate_sentence( model, sentence, german, english, device, max_length=50, trans=trans)\n",
        "print(\"German: \", sentence)\n",
        "print(\"Model translation: \", translated_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}